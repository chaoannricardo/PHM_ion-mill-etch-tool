{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5804ce5",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d82ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# local path\n",
    "# FAULT_DATA_DIR = \"D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\phm_etching_01M01-02\\\\M02_Groups_same_length\\\\fault\"\n",
    "# HEALTHY_DATA_DIR = \"D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\phm_etching_01M01-02\\\\M02_Groups_same_length\\\\healthy\"\n",
    "\n",
    "# server path\n",
    "FAULT_DATA_DIR = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\phm_etching_01M01-02\\\\M02_Groups_recipe_67_same_length\\\\fault\\\\\"\n",
    "HEALTHY_DATA_DIR = \"C:\\\\Users\\\\User\\Desktop\\\\Ricardo\\\\phm_etching_01M01-02\\\\M02_Groups_recipe_67_same_length\\\\healthy\\\\\"\n",
    "HEALTHY_DATA_NUM = 51\n",
    "MODEL_SAVE_PATH = \"C:\\\\Users\\\\User\\Desktop\\\\Ricardo\\\\phm_etching_01M01-02\\\\Models\\\\210619_LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b4d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 79.03it/s]\n",
      "100%|██████████| 51/51 [00:01<00:00, 43.50it/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(FAULT_DATA_DIR)\n",
    "\n",
    "# read in fault data\n",
    "for fileIndex, file in enumerate(tqdm(os.listdir())):\n",
    "    if fileIndex == 0:\n",
    "        data_all = pd.read_csv(file, encoding=\"utf8\")\n",
    "    else:\n",
    "        data_import = pd.read_csv(file, encoding=\"utf8\")\n",
    "        data_all = data_all.append(data_import, ignore_index = True)\n",
    "        \n",
    "os.chdir(HEALTHY_DATA_DIR)\n",
    "\n",
    "healthy_file_list = os.listdir()\n",
    "np.random.shuffle(healthy_file_list)\n",
    "\n",
    "for fileIndex, file in enumerate(tqdm(healthy_file_list[:HEALTHY_DATA_NUM])):\n",
    "    data_import = pd.read_csv(file, encoding=\"utf8\")\n",
    "    data_all = data_all.append(data_import, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f407f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "SEQ_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3249183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets with given sequence length\n",
    "def sliding_windows(data, seq_length, selected_column):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    # calculate max min for each column\n",
    "    data_temp = data.drop(columns=['time', 'Tool', 'Lot', 'Lot-runnum'])\n",
    "    \n",
    "    column_list = data_temp.columns.tolist()\n",
    "    max_value_list = data_temp.max().tolist()\n",
    "    min_value_list = data_temp.min().tolist()\n",
    "    \n",
    "    # normalize the data\n",
    "    for column in data.columns:\n",
    "        if column in ['time', 'Tool', 'Lot', 'Lot-runnum']:\n",
    "            continue\n",
    "            \n",
    "        # x ′ = ( x − x m i n ) / ( x m a x − x m i n )\n",
    "        column_index = column_list.index(column)\n",
    "        data[column] = data[column].apply(lambda x: \\\n",
    "                                          (x - min_value_list[column_index]) / (max_value_list[column_index] - \\\n",
    "                                                                   min_value_list[column_index]))\n",
    "        \n",
    "#     print(data.head(5))\n",
    "\n",
    "    for i in tqdm(range(len(data)-seq_length-1)):\n",
    "        # create x and y \n",
    "        data_x = data.iloc[i:(i+seq_length), :]\n",
    "        data_y = data.loc[[(i+seq_length)], [selected_column]]\n",
    "        \n",
    "        # skip if contains for than two Lot-runnum and np.nan appears in y\n",
    "        if data_x[\"Lot-runnum\"].nunique() > 1:\n",
    "            continue\n",
    "            \n",
    "        # drop nan value\n",
    "        if data_y[selected_column].isnull().values.any() or\\\n",
    "        data_x.isnull().values.any() :\n",
    "            continue\n",
    "        \n",
    "        # drop unneccessary columns\n",
    "        data_x.drop(columns=['time', 'Tool', 'Lot', 'TTF_FlowCool Pressure Dropped Below Limit',\n",
    "       'TTF_Flowcool Pressure Too High Check Flowcool Pump', 'TTF_Flowcool leak', 'Lot-runnum'],\n",
    "                     inplace=True)\n",
    "        \n",
    "        x.append(data_x)\n",
    "        y.append(data_y)\n",
    "        \n",
    "\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65e8cb",
   "metadata": {},
   "source": [
    "# Pytorch Reference\n",
    "#### CUDA Implementation Reference\n",
    "* https://pytorch.org/docs/stable/generated/torch.Tensor.to.html\n",
    "* https://pytorch.org/docs/stable/tensors.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html\n",
    "\n",
    "#### Saving & Load Model Reference:\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71437ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# activate cuda\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab152cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = SEQ_LENGTH\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        \n",
    "        out = self.fc(h_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49071823",
   "metadata": {},
   "source": [
    "# TTF_FlowCool Pressure Dropped Below Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3aca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102539 [00:00<?, ?it/s]c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "100%|██████████| 102539/102539 [02:25<00:00, 705.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47673, 500, 21) (47673, 1)\n"
     ]
    }
   ],
   "source": [
    "SELECTED_COLUMN = 'TTF_FlowCool Pressure Dropped Below Limit'\n",
    "x, y = sliding_windows(data_all, SEQ_LENGTH, SELECTED_COLUMN)\n",
    "y = np.squeeze(y, axis=2)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd445ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31940, 500, 21]) torch.Size([31940, 1]) \n",
      " torch.Size([15733, 500, 21]) torch.Size([15733, 1])\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(y) * 0.67)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "\n",
    "if is_cuda:\n",
    "    dataX = Variable(torch.Tensor(np.array(x)).cuda(device=0))\n",
    "    dataY = Variable(torch.Tensor(np.array(y)).cuda(device=0))\n",
    "\n",
    "    trainX = Variable(torch.Tensor(np.array(x[0:train_size])).cuda(device=0))\n",
    "    trainY = Variable(torch.Tensor(np.array(y[0:train_size])).cuda(device=0))\n",
    "\n",
    "    testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])).cuda(device=0))\n",
    "    testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])).cuda(device=0))\n",
    "else:\n",
    "    dataX = Variable(torch.Tensor(np.array(x)))\n",
    "    dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "    trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "    trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "    testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "    testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "\n",
    "print(trainX.shape, trainY.shape, \"\\n\", testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c93985",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 11.00 GiB total capacity; 8.61 GiB already allocated; 0 bytes free; 9.28 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-38d852718863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#     print(trainY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 11.00 GiB total capacity; 8.61 GiB already allocated; 0 bytes free; 9.28 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 21\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "if is_cuda:\n",
    "    lstm = lstm.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm(trainX)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY)\n",
    "    \n",
    "#     print(outputs)\n",
    "#     print(trainY)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        \n",
    "# save model\n",
    "torch.save(lstm.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "lstm.eval()\n",
    "train_predict = lstm(dataX)\n",
    "\n",
    "data_predict = train_predict.data.numpy()\n",
    "dataY_plot = dataY.data.numpy()\n",
    "\n",
    "# data_predict = sc.inverse_transform(data_predict)\n",
    "# dataY_plot = sc.inverse_transform(dataY_plot)\n",
    "\n",
    "plt.figure(figsize=(40,10))\n",
    "\n",
    "plt.axvline(x=train_size, c='r', linestyle='--')\n",
    "\n",
    "plt.plot(dataY_plot, color=\"blue\", linewidth=2)\n",
    "plt.plot(data_predict, color=\"red\", linewidth=2)\n",
    "plt.suptitle('Time-Series Prediction')\n",
    "plt.savefig(\"C:/Users/User/Desktop/Ricardo/PHM_ion-mill-etch-tool/Graphs/TTF_FlowCoolPressureDroppedBelowLimit.svg\", format='svg', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
